---
title: "Interpreting Topic Models in Byte-Pair Encoding Space"
collection: publications
category: published
permalink: /publication/2025-01-01-COLING
date: 2025-01-01
venue: 'COLING'
paperurl: 'https://aclanthology.org/2025.coling-main.720.pdf'
citation: 'Jia Peng Lim and Hady Lauw. 2025. Interpreting Topic Models in Byte-Pair Encoding Space. In Proceedings of the 31st International Conference on Computational Linguistics, pages 10810â€“10838, Abu Dhabi, UAE. Association for Computational Linguistics.'
---
TLDR: We can try to use Byte Pair Encoding in topic models if we know how to evaluate it.
<details>
<summary>Abstract</summary>
Byte-pair encoding (BPE) is pivotal for processing text into chunksize tokens, particularly in Large Language Model (LLM). From a topic modeling perspective, as these chunksize tokens might be mere parts of valid words, evaluating and interpreting these tokens for coherence is challenging. Most, if not all, of coherence evaluation measures are incompatible as they benchmark using valid words. We propose to interpret the recovery of valid words from these tokens as a ranking problem and present a model-agnostic and training-free recovery approach from the topic-token distribution onto a selected vocabulary space, following which we could apply existing evaluation measures. Results show that topic sets recovered from BPE vocabulary space are coherent.
</details>