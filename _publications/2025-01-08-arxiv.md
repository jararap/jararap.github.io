---
title: "A partition cover approach to tokenization"
collection: publications
category: arxiv
permalink: /publication/2025-01-08-arxiv
date: 2025-01-08
venue: 'arXiv preprint'
paperurl: 'https://arxiv.org/pdf/2501.06246'
citation: 'Jia Peng Lim, Davin Choo and Hady Lauw. "A partition cover approach to tokenization." arXiv preprint arXiv:2501.06246 (2025).'
---
TLDR: Tokenization is like vertex cover. GreedTok has better compression than Byte Pair Encoding.
<details>
<summary>Abstract</summary>
Tokenization is the process of encoding strings into tokens from a fixed vocabulary of size k  and is widely utilized in Natural Language Processing applications. The leading tokenization algorithm today is Byte Pair Encoding (BPE), which formulates the tokenization problem as a compression problem and tackles it by performing sequences of merges. In this work, we formulate tokenization as an optimization objective, show that it is NP-hard via a simple reduction from vertex cover, and propose a polynomial-time greedy algorithm GreedTok. Our formulation naturally relaxes to the well-studied weighted maximum coverage problem which has a simple (1-1/e)-approximation algorithm GreedWMC. Through empirical evaluations on real-world corpora, we show that GreedTok outperforms BPE, while achieving a comparable objective score as GreedWMC (which could have achieved a higher score due to relaxation).
</details>